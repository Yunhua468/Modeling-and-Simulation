\documentclass{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{comment}

\title{mit math lectures 22 and 23}
\author{Yunhua Zhao}
\date{\today}
\begin{document}
\maketitle

\section{Chapter 22 Expectation I} 
\subsection{Expectation Definition}
The expected value(also known as average or mean) of a r.v. R over a Probability space $S$ is denoted by:
$E_x(R)=\sum ^{w \in S}R(w)P_r(w)$  \\
\textbf{Def median:} the median of a r.v. R is the $x\in Range(R)$, such that 
$$P_r(R<x)\leqslant 1/2$$
$$P_r(R>x)< 1/2$$
\textbf{Thm:} $E_x(R)=\sum_{min}^{x\in Range(R)}xP_r(R=x)$  \\

\textbf{Note: think about what you want, max return or max probability}\\
\textbf{Cor:} If the random variable has a range of naturals, then 
$$ E_x(R)=\sum_{i=1}^{\infty}iP_r(R==i) $$
\textbf{Thm:} If R the random variable has a range of naturals, then
$$  E_x(R)=\sum_{i=1}^{\infty}P_r(R>i) $$

\subsection{Linearity of Expectation}
\textbf{Thm:} For any random variables $R_1$ and $R_2$ on the probability space $S$, 
$$ E_x(R_1+R_2)=E_x(R_1)+E_x(R_2) $$
\textbf{Cor:} $\forall k\in n$ (n means natural number), $k$ is random variables $R_1$, $R_2$,... on the probability space $S$,
$$ E_x(R_1+R_2+...+R_k)=E_x(R_1)+E_x(R_2)+...+E_x(R_k) $$

\textbf{Note: No Independence needed}\\

\section{Chapter 23 Expectation II} 
\subsection{Thm 1. Another way to calculate expectation}
\textbf{Thm 1:} Given a Probability space $S$, events $A_1$, $A_2$, ... $A_n$ $\subseteq S$, then the expected number of these events to occur is $\sum_{i=1}^{n}P_r(A_i)$, where $P_r(A_i)$ is the probability $A_i$ occurs \\
\textbf{Note: Some time use theorem 1 is much easier than expectation original definition}\\

\subsection{Thm 2. bound of the probability of at least one event occurs}
\textbf{Thm 2:} $P_r(T \geq 1) \leq E_x(T) $, means the probability of at least one event occurs is bounded by the expectation of events occur.  \\
\textbf{Cor:} $P_r(T \geq 1) \leq \sum_{i=1}^{n}P_r(A_i) $  \\

\subsection{Thm 3. Murphy's law}
Given mutual independent events, $A_1,..., A_n$, then the prob that none of them occurs is $P_r(T=0)\leq e^{-E_x(T)}$  \\
when proof uses $\forall x$, $1-x \leq e^{-x}$  \\
\textbf{Cor:} If we expect 10 or more mutually independent events to occur, the prob that no event occurs is $\leq e^{-10}<1/22000$  \\

\subsection{Thm 4. Product Rule for Expectation}
For any \textbf{independent} r.v.'s $R_1$, $R_2$, 
$$ E_x(R_1R_2)=E_x(R_1)E_x(R_2) $$
\textbf{Noe ex: 6 sizes dice} $E_x(R_1R_1)=E_x(R_1^2)=\sum_{i=1}^{6}i^2P_r(R_1=1)=1/6(1+4+9+16+25+36)=15(1/6)\neg (3 1/2)^2=E_x^2(R_1)$  \\
\textbf{Cor:} If  $R_1$,..., $R_n$ are mutually independent, then $$ E_x(R_1...R_n)=E_x(R_1)...E_x(R_n) $$
\textbf{Cor:} For any constants a,b and any rv R, $$ E_x(aR+b)=aE_x(R)+b $$
\textbf{Cor??} $E_x(1/R)=1/E_x(R)$  \textbf{No, because if $R\in(1,-1)$, then $E_x(1/R)=0$, so false} \\
\textbf{Cor??} Given independent rv's $R$ and $T$, if $E_x(R/T)>1$, then $E_x(R)>E_x(T)$,  \textbf{No No No} \\

\subsection{Deviation}
\textbf{Def:} The variance of a rv R is denoted by $$Var(R)=E_x((R-E_x(R))^2)$$
deviation from mean: $R-E_x(R)$  \\
square deviation $(R-E_x(R))^2$  \\
var = Expectation of square deviation  \\































\end{document}